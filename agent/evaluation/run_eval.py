import os
import sys
import uuid
from typing import Dict, Any

import dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langsmith import evaluate, Client
from pydantic import BaseModel, Field

from agent import RAGAgent

# 1. è·å–å½“å‰è„šæœ¬ (run_eval.py) æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
current_script_path = os.path.abspath(__file__)
# ç»“æœ: D:\code\test_langchain\...\evaluation\run_eval.py

# 2. è·å–é¡¹ç›®æ ¹ç›®å½• (å³ evaluation çš„ä¸Šä¸€çº§)
project_root = os.path.dirname(os.path.dirname(current_script_path))
# ç»“æœ: D:\code\test_langchain\Knowledge-base-question-answering-system

# --- å…³é”®ä¿®å¤ä»£ç  ---
# 3. å¼ºåˆ¶æŠŠå½“å‰çš„å·¥ä½œç›®å½• (CWD) åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•
# è¿™æ ·ï¼Œä½ ä»£ç é‡Œæ‰€æœ‰çš„ç›¸å¯¹è·¯å¾„ (å¦‚ "./data", "./db") éƒ½ä¼šåŸºäºæ ¹ç›®å½•å¯»æ‰¾
os.chdir(project_root)
print(f"ğŸ”„ å·²å°†å·¥ä½œç›®å½•åˆ‡æ¢è‡³: {os.getcwd()}")

# 4. æŠŠæ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„ (è§£å†³ import agent æŠ¥é”™)
sys.path.append(project_root)

dotenv.load_dotenv()
agent = RAGAgent()


class Comment(BaseModel):
    score: int = Field(
        description="å¯¹æ¨¡å‹çš„å›ç­”è¿›è¡Œæ‰“åˆ†ï¼Œä»0åˆ°100åˆ†ï¼Œ100ä¸ºå›å¤å‡†ç¡®ã€‚",
        ge=0, le=100
    )
    comment: str = Field(
        description="å¯¹æ¨¡å‹å›ç­”çš„ç®€çŸ­è¯„ä»·ã€‚"
    )


def bridge_func(inputs: dict) -> dict:
    question = inputs["question"]
    thread_id = str(uuid.uuid4())   # ä½¿ç”¨æ–°ä¼šè¯ï¼Œé˜²æ­¢è®°å¿†å½±å“

    result = agent.invoke(question, thread_id)
    if hasattr(result, "answer"):
        return {
            "reason": result.reason,
            "answer": result.answer,
            "source": getattr(result, "sources", ""),
        }
    else:
        return {
            "answer": str(result)
        }


llm = ChatGoogleGenerativeAI(model=os.getenv("GEMINI_MODEL"))
eval_llm = llm.with_structured_output(Comment)


def evaluator(run, example) -> Dict[str, Any]:

    """
    :param run: bridge_func è¿”å›çš„ç»“æœ
    :param example: æµ‹è¯•é›†ç¤ºä¾‹åŠç­”æ¡ˆ
    :return:
    """
    question = example.inputs.get("question")
    reference_answer = example.outputs.get("answer", "")
    outputs = getattr(run, "outputs", {}) or {}
    llm_answer = outputs.get("answer", "")

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„åˆ†å‘˜ã€‚
    
    é—®é¢˜ï¼š{question}
    æ ‡å‡†ç­”æ¡ˆï¼š{reference_answer}
    AIçš„å›ç­”ï¼š{llm_answer}
    
    è¯·åˆ¤æ–­ AI çš„å›ç­”æ˜¯å¦åœ¨äº‹å®å±‚é¢ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´ã€‚
    å¿½ç•¥æªè¾å·®å¼‚ã€‚
    """
    comment = eval_llm.invoke(prompt)
    return {
        "score": comment.score,
        "comment": comment.comment,
    }


dataset_name = "General-Agent-Benchmark"
dataset_data = [
    # --- ç¬¬ä¸€ç±»ï¼šå¸¸è¯†ä¸äº‹å® (General Knowledge) ---
    {
        "input": "Python è¯­è¨€æ˜¯è°å‘æ˜çš„ï¼Ÿ",
        "output": "Guido van Rossum"
    },
    {
        "input": "å¤ªé˜³ç³»ä¸­ä½“ç§¯æœ€å¤§çš„è¡Œæ˜Ÿæ˜¯å“ªä¸€é¢—ï¼Ÿ",
        "output": "æœ¨æ˜Ÿ (Jupiter)"
    },
    {
        "input": "æ³°å¦å°¼å…‹å·æ˜¯åœ¨å“ªä¸€å¹´æ²‰æ²¡çš„ï¼Ÿ",
        "output": "1912å¹´"
    },

    # --- ç¬¬äºŒç±»ï¼šé€»è¾‘ä¸æ•°å­¦ (Logic & Math) ---
    {
        "input": "æˆ‘æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰1ä¸ªï¼Œåˆä¹°äº†5ä¸ªï¼Œç°åœ¨æˆ‘æœ‰å‡ ä¸ªè‹¹æœï¼Ÿ",
        "output": "7ä¸ª"
    },
    {
        "input": "å¦‚æœæ˜¨å¤©æ˜¯å‘¨äºŒï¼Œé‚£ä¹ˆåå¤©æ˜¯å‘¨å‡ ï¼Ÿ",
        "output": "å‘¨äº”"
    },
    {
        "input": "25 çš„å¹³æ–¹æ ¹æ˜¯å¤šå°‘ï¼Ÿ",
        "output": "5"
    },

    # --- ç¬¬ä¸‰ç±»ï¼šå¤šæ­¥æ¨ç† (Complex Reasoning) ---
    # è¿™ç±»é—®é¢˜é€šå¸¸éœ€è¦ Agent è¿›è¡Œæœç´¢æˆ–æ·±å±‚æ€è€ƒ
    {
        "input": "ç°ä»»ç¾å›½æ€»ç»Ÿçš„å‡ºç”Ÿåœ°æ˜¯å“ªä¸ªå·ï¼Ÿ",
        "output": "å–å†³äºå½“å‰æ—¶é—´ç‚¹ (ä¾‹å¦‚æ‹œç™»æ˜¯å®¾å¤•æ³•å°¼äºšå·ï¼Œç‰¹æœ—æ™®æ˜¯çº½çº¦å·)"
    },
    {
        "input": "ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹ç³»åˆ—ç”µå½±ä¸­æ‰®æ¼”èµ«æ•çš„æ¼”å‘˜ï¼Œå¥¹ä¹Ÿæ˜¯å“ªéƒ¨è¿ªå£«å°¼çœŸäººç”µå½±çš„ä¸»è§’ï¼Ÿ",
        "output": "è‰¾ç›Â·æ²ƒç‰¹æ£® (Emma Watson)ï¼Œå¥¹ä¹Ÿæ˜¯ã€Šç¾å¥³ä¸é‡å…½ã€‹çš„ä¸»è§’ã€‚"
    },

    # --- ç¬¬å››ç±»ï¼šç®€å•çš„æŒ‡ä»¤éµå¾ª (Instruction) ---
    {
        "input": "è¯·æŠŠ 'Hello World' ç¿»è¯‘æˆæ³•è¯­ï¼Œåªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦åºŸè¯ã€‚",
        "output": "Bonjour le monde"
    },
    {
        "input": "å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ Python å‡½æ•°ã€‚",
        "output": "def fib(n): ..."
    }
]

client = Client()
if client.has_dataset(dataset_name=dataset_name):
    print(f"ğŸ”„ æ•°æ®é›† '{dataset_name}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤é‡å»ºä»¥ç¡®ä¿æ•°æ®æœ€æ–°...")
    client.delete_dataset(dataset_name=dataset_name)

print(f"ğŸ“¦ æ­£åœ¨åˆ›å»ºæ•°æ®é›†: {dataset_name} ...")
dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="åŒ…å«å¸¸è¯†ã€æ•°å­¦ã€é€»è¾‘å’Œå¤šæ­¥æ¨ç†çš„é€šç”¨ Agent æµ‹è¯•é›†"
)

inputs = [{"question": item["input"]} for item in dataset_data]
outputs = [{"answer": item["output"]} for item in dataset_data]

client.create_examples(
    inputs=inputs,
    outputs=outputs,
    dataset_id=dataset.id
)

if __name__ == '__main__':
    eval_res = evaluate(
        bridge_func,
        data=dataset_name,
        evaluators=[evaluator],
        experiment_prefix="agent-v1-test",
        max_concurrency=1
    )
