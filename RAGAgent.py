import os
import uuid
from typing import TypedDict, List, Annotated, Literal, Optional

from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import add_messages, StateGraph
from pydantic import BaseModel, Field

from tools.rag_tool import rag_retriever


class RAGState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    documents: Optional[List[Document]]
    question: str   # Rewrite node ä¼šä¿®æ”¹å…³é”®è¯
    retry_count: int
    grade: Optional[str]      # "yes" or "no"


def retrieve(state: RAGState):
    question = state['question']
    docs = rag_retriever.invoke(question)
    return {"documents": docs}


class Grade(BaseModel):
    grade: Literal["yes", "no"] = Field(description="åªå›ç­” 'yes' or 'no'")

llm = ChatOpenAI(model=os.getenv("MODEL_NAME"))

def grade_documents(state: RAGState):
    question = state['question']
    documents = state['documents']
    template = PromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä¸ªè¯„å®¡å‘˜ã€‚
    è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š{question}
    è¿™æ˜¯æ£€ç´¢çš„æ–‡æ¡£ï¼š{document}
    è¯·åˆ¤æ–­è¿™ä¸ªæ–‡æ¡£çœŸå›ç­”äº†é—®é¢˜å—ï¼Ÿ
    """)

    structured_llm = llm.with_structured_output(Grade)
    grades = []
    for document in documents:
        prompt = template.format(question=question, document=document.page_content)
        res = structured_llm.invoke(prompt)
        grades.append(res.grade)

    reduced_docs = []
    for item in list(zip(documents, grades)):
        doc, grade = item
        if grade == "yes":
            reduced_docs.append(doc)

    if not reduced_docs:
        return {
            "documents": [],
            "grade": "no"
        }
    return {
        "documents": reduced_docs,
        "grade": "yes"
    }


def generate(state: RAGState):
    question = state['question']
    documents = state['documents']

    if not documents:
        print("---ç”Ÿæˆå›å¤: æ— èµ„æ–™ï¼Œå›å¤ä¸çŸ¥é“---")
        final_answer = "æŠ±æ­‰ï¼Œç»è¿‡å¤šæ¬¡æ£€ç´¢ï¼Œæˆ‘ä¾ç„¶æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸è¯¥é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚å»ºè®®æ‚¨å°è¯•æ›´æ¢å…³é”®è¯æˆ–æŸ¥é˜…å…¶ä»–æ¥æºã€‚"
        return {"messages": [AIMessage(content=final_answer)]}

    docs = "\n\n".join(doc.page_content for doc in documents)
    prompt = f"""
    è¿™æ˜¯ç”¨æˆ·çš„æé—®ï¼š{question}
    è¿™æ˜¯ RAG æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š{docs}
    è¯·ä½ æ ¹æ®è¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    """
    result = llm.invoke(prompt)
    return {"messages": [result]}


def rewrite(state: RAGState):
    question = state['question']
    current_attempt = state.get("retry_count", 0)
    prompt = f"""
    ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{question}
    åˆæ¬¡æ£€ç´¢æ²¡æœ‰å‘ç°ç›¸å…³ä¿¡æ¯ã€‚
    è¯·åˆ†æé—®é¢˜æ„å›¾ï¼Œè¾“å‡ºä¸€ä¸ªä¼˜åŒ–åçš„ã€æ›´é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ã€‚
    åªè¾“å‡ºå…³é”®è¯ï¼Œä¸è¦åŒ…å«è§£é‡Šã€‚
    """
    result = llm.invoke(prompt)
    print(f"ğŸ”„ æ”¹å†™é—®é¢˜: {question} -> {result.content} (ç¬¬ {current_attempt + 1} æ¬¡å°è¯•)")
    return {
        "question": result.content,
        "retry_count": current_attempt + 1
    }


graph = StateGraph(RAGState)
graph.add_node("rag", retrieve)
graph.add_node("grade", grade_documents)
graph.add_node("rewrite", rewrite)
graph.add_node("generate", generate)
graph.set_entry_point("rag")
graph.add_edge("rag", "grade")

def grade_continue(state: RAGState):
    grade = state['grade']
    retry_count = state['retry_count']
    if grade == "yes":
        return "generate"
    else:
        if retry_count < 3:
            return "rewrite"
        else:
            return "generate"

graph.add_conditional_edges("grade", grade_continue)
graph.add_edge("rewrite", "rag")
graph.add_edge("generate", "__end__")
app = graph.compile()

@tool
def call_rag_expert(task: str) -> str:
    """
    è¿™æ˜¯ä¸€ä¸ªå…·å¤‡ã€è‡ªæˆ‘çº é”™ã€‘å’Œã€æ·±åº¦æ£€ç´¢ã€‘èƒ½åŠ›çš„é«˜çº§çŸ¥è¯†æŸ¥è¯¢å·¥å…·ã€‚

    é€‚ç”¨åœºæ™¯ï¼š
    1. å½“ä½ éœ€è¦å›ç­”å…·ä½“çš„äº‹å®æ€§é—®é¢˜ã€å†å²æ•°æ®æˆ–ä¸“ä¸šçŸ¥è¯†æ—¶ã€‚
    2. å½“æ ‡å‡†ç­”æ¡ˆéœ€è¦é«˜å‡†ç¡®åº¦ï¼Œä¸”å…è®¸ç¨é•¿çš„æ£€ç´¢æ—¶é—´æ—¶ï¼ˆè¯¥å·¥å…·ä¼šåœ¨å†…éƒ¨è¿›è¡Œå¤šè½®éªŒè¯å’Œé‡è¯•ï¼‰ã€‚

    å‚æ•°è¦æ±‚ï¼š
    - task: æœç´¢æŸ¥è¯¢è¯­å¥ã€‚æ³¨æ„ï¼šè¿™å¿…é¡»æ˜¯ä¸€ä¸ªã€ç‹¬ç«‹å®Œæ•´ã€‘çš„å¥å­ã€‚
      å¦‚æœç”¨æˆ·çš„å¯¹è¯åŒ…å«æŒ‡ä»£è¯ï¼ˆå¦‚â€œå®ƒâ€ã€â€œé‚£ä¸ªäººâ€ï¼‰ï¼Œè¯·åœ¨è°ƒç”¨æ­¤å·¥å…·å‰å°†å…¶æ›¿æ¢ä¸ºå…·ä½“çš„å®ä½“åç§°ã€‚
      ä¾‹å¦‚ï¼šå°† "ä»–çš„è‚¡ä»·æ˜¯å¤šå°‘" æ”¹å†™ä¸º "ç‰¹æ–¯æ‹‰çš„è‚¡ä»·æ˜¯å¤šå°‘"ã€‚
    """
    inputs = {
        "messages": [HumanMessage(content=task)],
        "question": task,
        "retry_count": 0,
    }
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}

    result = app.invoke(inputs, config)

    final_msg = result["messages"][-1]
    return final_msg.content
