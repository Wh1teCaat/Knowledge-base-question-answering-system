import os
import uuid
from typing import TypedDict, List, Annotated, Literal, Optional

from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import add_messages, StateGraph
from pydantic import BaseModel, Field

from tools.rag_tool import rag_retriever


class RAGState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    documents: Optional[List[Document]]
    question: str   # Rewrite node ä¼šä¿®æ”¹å…³é”®è¯
    retry_count: int
    grade: Optional[str]      # "yes" or "no"


def retrieve(state: RAGState):
    question = state['question']
    docs = rag_retriever.invoke(question)
    return {"documents": docs}


class Grade(BaseModel):
    grade: Literal["yes", "no"] = Field(description="åªå›ç­” 'yes' or 'no'")

llm = ChatOpenAI(model=os.getenv("MODEL_NAME"))

def grade_documents(state: RAGState):
    question = state['question']
    documents = state['documents']
    template = PromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä¸ªè¯„å®¡å‘˜ã€‚
    è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š{question}
    è¿™æ˜¯æ£€ç´¢çš„æ–‡æ¡£ï¼š{document}
    è¯·åˆ¤æ–­è¿™ä¸ªæ–‡æ¡£çœŸå›ç­”äº†é—®é¢˜å—ï¼Ÿ
    """)

    structured_llm = llm.with_structured_output(Grade)
    grades = []
    for document in documents:
        prompt = template.format(question=question, document=document.page_content)
        res = structured_llm.invoke(prompt)
        grades.append(res.grade)

    reduced_docs = []
    for item in list(zip(documents, grades)):
        doc, grade = item
        if grade == "yes":
            reduced_docs.append(doc)

    if not reduced_docs:
        return {
            "documents": [],
            "grade": "no"
        }
    return {
        "documents": reduced_docs,
        "grade": "yes"
    }


def generate(state: RAGState):
    question = state['question']
    documents = state['documents']

    if not documents:
        print("---ç”Ÿæˆå›å¤: æ— èµ„æ–™ï¼Œå›å¤ä¸çŸ¥é“---")
        final_answer = "æŠ±æ­‰ï¼Œç»è¿‡å¤šæ¬¡æ£€ç´¢ï¼Œæˆ‘ä¾ç„¶æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸è¯¥é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚å»ºè®®æ‚¨å°è¯•æ›´æ¢å…³é”®è¯æˆ–æŸ¥é˜…å…¶ä»–æ¥æºã€‚"
        return {"messages": [AIMessage(content=final_answer)]}

    docs = "\n\n".join(doc.page_content for doc in documents)
    prompt = f"""
    è¿™æ˜¯ç”¨æˆ·çš„æé—®ï¼š{question}
    è¿™æ˜¯ RAG æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š{docs}
    è¯·ä½ æ ¹æ®è¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    """
    result = llm.invoke(prompt)
    return {"messages": [result]}


def rewrite(state: RAGState):
    question = state['question']
    current_attempt = state.get("retry_count", 0)
    prompt = f"""
    ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{question}
    åˆæ¬¡æ£€ç´¢æ²¡æœ‰å‘ç°ç›¸å…³ä¿¡æ¯ã€‚
    è¯·åˆ†æé—®é¢˜æ„å›¾ï¼Œè¾“å‡ºä¸€ä¸ªä¼˜åŒ–åçš„ã€æ›´é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ã€‚
    åªè¾“å‡ºå…³é”®è¯ï¼Œä¸è¦åŒ…å«è§£é‡Šã€‚
    """
    result = llm.invoke(prompt)
    print(f"ğŸ”„ æ”¹å†™é—®é¢˜: {question} -> {result.content} (ç¬¬ {current_attempt + 1} æ¬¡å°è¯•)")
    return {
        "question": result.content,
        "retry_count": current_attempt + 1
    }


graph = StateGraph(RAGState)
graph.add_node("rag", retrieve)
graph.add_node("grade", grade_documents)
graph.add_node("rewrite", rewrite)
graph.add_node("generate", generate)
graph.set_entry_point("rag")
graph.add_edge("rag", "grade")

def grade_continue(state: RAGState):
    grade = state['grade']
    retry_count = state['retry_count']
    if grade == "yes":
        return "generate"
    else:
        if retry_count < 3:
            return "rewrite"
        else:
            return "generate"

graph.add_conditional_edges("grade", grade_continue)
graph.add_edge("rewrite", "rag")
graph.add_edge("generate", "__end__")
app = graph.compile()

@tool
def call_rag_expert(task: str) -> str:
    """
    ã€å†…éƒ¨çŸ¥è¯†åº“ä¸“å®¶ã€‘

    é€‚ç”¨åœºæ™¯ï¼š
    1. æŸ¥è¯¢ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚åœ°è´¨ã€æ³•å¾‹ã€å…¬å¸è§„ç« ç­‰æœ¬åœ°æ–‡æ¡£ï¼‰ã€‚
    2. æŸ¥è¯¢å†å²æ¡£æ¡ˆæˆ–å·²æœ‰çš„å›ºå®šäº‹å®ã€‚

    âŒ ä¸¥ç¦ç”¨äºï¼š
    1. æŸ¥è¯¢å®æ—¶ä¿¡æ¯ï¼ˆå¦‚ä»Šå¤©çš„å¤©æ°”ã€ç°åœ¨çš„è‚¡ä»·ï¼‰ã€‚
    2. æŸ¥è¯¢æœªæ¥çš„é¢„æµ‹ï¼ˆå¦‚2025å¹´çš„äº‹æƒ…ï¼‰ã€‚
    3. é—²èŠã€‚
    """
    inputs = {
        "messages": [HumanMessage(content=task)],
        "question": task,
        "retry_count": 0,
    }
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}

    result = app.invoke(inputs, config)

    final_msg = result["messages"][-1]
    return final_msg.content
